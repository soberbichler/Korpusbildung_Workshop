{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification for topic-specific newspaper collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the process of categorizing text into pre-defined groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of given categories based on the research question. This automated classification of text into predefined categories is an important method for managing and processing a large number of newspaper clippings. This also applies to subcorpora for a specific research topic (e.g. migration). The aim of this notebook is to train a model using your previously manually created training/test corpus and to use this model to get an overview of the category distribution throughout your collection (see figure below). Another goal is to export your categorized data for further analysis. This makes it possible to examine, for example, the advertisement about a specific topic.\n",
    "\n",
    "This notebook was used with a collection for the case study on emigration (1850-1950) and shows how a model can be trained to classify topic-specific collections. For the training/testing corpus, a collection with the keywords \"Auswander*\", \"Ausgewanderte\", \"Emigrant*\", \"Emigrierte\", \"Emigration\", \"Kolonist*\", and \"Ansiedler*\" (all different German words for emigrants or emigration) have been created. In addition, information on the pre-defined gropus (news, ads, culture...) were added using numbers between one and ten. \n",
    "\n",
    "For classification, topic modelling (LDA) was chosen because it showed the best performance in classification (after experiments with word embeddings or LDA and word embeddings combined). LDA provides a way to group documents by topic and perform similarity searches and improve precision. Thanks to sklearn, it is relatively easy to test different classifiers for a given topic classification task. Logistic regression was chosen as binary classifier. \n",
    "\n",
    "*Following graph demonstrates the distribution of the pre-defined categories in newspaper clippings of seleceted Austrian Newspapers (sample of 1631 newspaper clippings) on the topic of emigration.* \n",
    "\n",
    "![Collection on the topic of Emigration](images/circle.PNG)\n",
    "\n",
    "\n",
    "Read more about <a href=\"https://monkeylearn.com/blog/introduction-to-topic-modeling/\" target=\"_blank\">Topic Modeling</a> and <a href=\"https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5\" target=\"_blank\">Logistic Regression Model Tuning</a>.\n",
    "\n",
    "Acknowledgments:\n",
    "\n",
    "This work has been inspired by a notebook on <a href=\"https://www.kaggle.com/vukglisovic/classification-combining-lda-and-word2vec\" target=\"_blank\">LDA and word embeddings</a> and several other soursces that provided help on how to buid models. This work was supported by the European Union's Horizon 2020 research and innovation programme under grant 770299 (NewsEye)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step...\n",
    "* [Prepare a small manually annotated collection](#1-bullet)\n",
    "* [Install packages in command line](#2-bullet)\n",
    "* [Import packages](#3-bullet)\n",
    "* [Import your manually annotated newspaper collection](#4-bullet)\n",
    "* [Clean and tokenize the text (pre-processing)](#5-bullet)\n",
    "* [Have a look at your data](#6-bullet)\n",
    "* [Use your dataset to create a training corpus and test corpus](#7-bullet)\n",
    "* [Create topic models using your training corpus](#8-bullet)\n",
    "* [Have a look at your topics](#9-bullet)\n",
    "* [Create the feature vector ](#10-bullet)\n",
    "* [Have a look at the top words for each category](#11-bullet)\n",
    "* [Classification and hyperparameter tuning](#12-bullet)\n",
    "* [Using the test corpus](#13-bullet)\n",
    "* [Logistic Regression](#14-bullet)\n",
    "* [Now it is time to make the classifications](#15-bullet)\n",
    "* [Calculate the score for each category as well as the overall score](#16-bullet)\n",
    "* [If your overall score is higher than 80 percent, you can start to use your whole collection](#17-bullet)\n",
    "* [Clean (pre-process) your whole collection](#18-bullet)\n",
    "* [Now it is time to make the classifications for the whole collection](#19-bullet)\n",
    "* [Create a dataframe with the results](#20-bullet)\n",
    "* [If you are satisfied with the results, you can save them in the form of your original file](#21-bullet)\n",
    "* [Visualize your results](#22-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a small manually annotated collection <a class=\"anchor\" id=\"1-bullet\"></a>\n",
    "\n",
    "This program uses annotations for evaluation and classification. Therefore, a manually annotated collection of  80 to 100 articles per category is needed to work with this program. To create this collection, the numbers 0 to 7 have been assignet to the articles, each number representing one newspaper category (ads, news, culture_literature_stories_letters, appeals_donations_information, crime, finance, statistic). When you create your own collection, make sure you create a representative collection of the whole search result. If you use a long time period, make sure all timer periods are represented in your small collection. \n",
    "The newspaper articles with the annotations should be saved as CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages in command line <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    " \n",
    "If you need help on how to pip install, have a look at this tutorial: https://packaging.python.org/tutorials/installing-packages/\n",
    "\n",
    "pip install gensim\n",
    "\n",
    "pip install PyLDAvis\n",
    "\n",
    "pip install spacy\n",
    "\n",
    "python -m spacy download de_core_web_sm\n",
    "\n",
    "pip install pandas\n",
    "\n",
    "pip install regex\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "pip install matplotlib\n",
    "\n",
    "pip install numpy\n",
    "\n",
    "pip install seaborn\n",
    "\n",
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "\n",
    "Before you can get started, you have to install and import some packages.\n",
    "\n",
    "#### Make sure you use the version 1.9.0 with smart_open: python -m pip install --upgrade smart_open==1.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c62255\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# preprocessing imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "# model imports\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LDA\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Frequency\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your manually annotated newspaper collection <a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "And have a look at your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(698, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>iiif_url</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19110626_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1911-06-26T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>3</td>\n",
       "      <td>Blumenjunge, Meidling. Sie erhalten jetzt\\nim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19110403_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1911-04-03T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>3</td>\n",
       "      <td>Wien—Amerika, Zentagasse. Vor der Auswanderung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19111027_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1911-10-27T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>3</td>\n",
       "      <td>Auskünfte.\\n Kolonie 1000, Viktor\\nM. Wenden S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id language  \\\n",
       "0  illustrierte_kronen_zeitung_krz19110626_articl...       de   \n",
       "1  illustrierte_kronen_zeitung_krz19110403_articl...       de   \n",
       "2  illustrierte_kronen_zeitung_krz19111027_articl...       de   \n",
       "\n",
       "                   date                 newspaper_id  \\\n",
       "0  1911-06-26T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "1  1911-04-03T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "2  1911-10-27T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "\n",
       "                                            iiif_url  relevancy  \\\n",
       "0  https://platform.newseye.eu/iiif/illustrierte_...          3   \n",
       "1  https://platform.newseye.eu/iiif/illustrierte_...          3   \n",
       "2  https://platform.newseye.eu/iiif/illustrierte_...          3   \n",
       "\n",
       "                                                text  \n",
       "0  Blumenjunge, Meidling. Sie erhalten jetzt\\nim ...  \n",
       "1  Wien—Amerika, Zentagasse. Vor der Auswanderung...  \n",
       "2  Auskünfte.\\n Kolonie 1000, Viktor\\nM. Wenden S...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/export_classification_emigration_new_06_07_2020_23_15.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of your annotated categories. \n",
    "Each category is assignet to a number: \n",
    "\n",
    "0 = Advertisements\n",
    " \n",
    "1 = News\n",
    "\n",
    "2 = Culture, Literature, Stories, and Letters\n",
    "\n",
    "3 = Appeals, Donations, and information\n",
    "\n",
    "4 = Crime\n",
    "\n",
    "6 = Finance\n",
    "\n",
    "7 = Statistic\n",
    "\n",
    "These categories where specifically chosen for a collection on emigration between 1850 and 1950. For this corpus, about 80 articles for each categorie were sufficient to train a model that delivers good results. However, it is important that the corpus is representative for this specific topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ddecf44fc8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.relevancy.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and tokenize the text (pre-processing) <a class=\"anchor\" id=\"5-bullet\"></a>\n",
    "\n",
    "Before you can start with the training fo the topic models, you have to clean the text of your newspaper articles. The follwing functions remove punctuations, lower case the text, remove stop words and lemmatize the text. \n",
    "\n",
    "#### Stop words: \n",
    "You can change the language used for the stop words. You can also add your own stop words or other words you would like to ignore. It helps to ignore your search keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean, tokenize, and lemmatize the data\n",
    "def initial_clean(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower() \n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('german')#change the language here\n",
    "# add your onw stop words\n",
    "stop_words.extend(['auswanderer', 'auswanderung', 'auswanderern'])\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] \n",
    "    except IndexError: \n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_stop_words(initial_clean(text)))\n",
    "\n",
    "df['tokenized'] = df['text'].apply(apply_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at your data <a class=\"anchor\" id=\"6-bullet\"></a>\n",
    "\n",
    "Check out, if everything went alright so far. Have a look at the number of words and their frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The number of unique words is 14683'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first get a list of all words\n",
    "all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(all_words)\n",
    "f\"The number of unique words is {len(fdist)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list: 698\n",
      "average document length: 72.55444126074498\n",
      "minimum document length: 4\n",
      "maximum document length: 602\n"
     ]
    }
   ],
   "source": [
    "# document length\n",
    "df['doc_len'] = df['tokenized'].apply(lambda x: len(x))\n",
    "doc_lengths = list(df['doc_len'])\n",
    "df.drop(labels='doc_len', axis=1, inplace=True)\n",
    "\n",
    "print(f\"length of list: {len(doc_lengths)}\")\n",
    "print(f\"average document length: {np.average(doc_lengths)}\")\n",
    "print(f\"minimum document length: {min(doc_lengths)}\")\n",
    "print(f\"maximum document length: {max(doc_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Use your dataset to create a training corpus and test corpus <a class=\"anchor\" id=\"7-bullet\"></a>\n",
    "\n",
    "Before we use our model on a bigger, unseen collection, we use our manual annotated dataset to train the models and classify the newspaper clippings. This helps to control the output (the annotations show if the automated classification has worked corretly) and to adapt the code in order to get the best results for your own collection. \n",
    "\n",
    "You can change the size of training and testing corpus by changing the number in following line: msk = np.random.rand(len(df)) < 0.899\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask of binary values\n",
    "np.random.seed(7070)\n",
    "msk = np.random.rand(len(df)) < 0.850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[msk]\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "test_df = df[~msk]\n",
    "test_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 599 99\n"
     ]
    }
   ],
   "source": [
    "print(len(df),len(train_df),len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure, all categories have the same size\n",
    "Therefore we shorten the training corpus to the number of the smallest category in the corpus. This is important so that the results are not distorted by over- or under-representation of a category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train_df.relevancy.value_counts().min()\n",
    "train_df = train_df.groupby('relevancy').head(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ddf0540c08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADoVJREFUeJzt3XuMXGd9xvHvg51wKzQJ2VhuTDBRTYBKzW0VqKJWgEkJTUWsCqqkiK5QWv8DJRGViktVVagXBalqyh9VJYtAtxLkQkpki1YByxAq2irN5gIkccAkCmAlsReaCxAEOPn1jzmhxuxmzs7OeLxvvx/JOnPec87O42T9zOt354xTVUiS1r7nTTuAJGk8LHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSI9Yfyyc79dRTa/PmzcfyKSVpzbvjjju+U1Uzw847poW+efNmFhYWjuVTStKal+Sbfc5zyUWSGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUiGN6Y9FKbd7xrxP9+g9dfclEv/5azr+Ws4P5hzH/c1ur+Z2hS1IjLHRJasTQQk9yVpK7j/j1ZJKrkpySZE+S/d325GMRWJK0tKGFXlVfq6pzquoc4HzgKeBmYAewt6q2AHu7fUnSlKx0yWUr8EBVfRO4FJjvxueBbeMMJklamZUW+mXAdd3jDVX1CEC3PW2pC5JsT7KQZGFxcXH0pJKk59S70JOcCLwN+NRKnqCqdlbVbFXNzswM/Xx2SdKIVjJDfytwZ1Ud7PYPJtkI0G0PjTucJKm/lRT65fzfcgvAbmCuezwH7BpXKEnSyvUq9CQvAi4CPn3E8NXARUn2d8euHn88SVJfvW79r6qngJcdNfZdBu96kSQdB7xTVJIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjehV6ElOSnJTkvuT7Evya0lOSbInyf5ue/Kkw0qSltd3hv4R4JaqejVwNrAP2AHsraotwN5uX5I0JUMLPclLgd8ArgWoqh9X1ePApcB8d9o8sG1SISVJw/WZoZ8JLAIfT3JXko8meTGwoaoeAei2py11cZLtSRaSLCwuLo4tuCTpZ/Up9PXAecA/VtW5wA9YwfJKVe2sqtmqmp2ZmRkxpiRpmD6FfgA4UFW3dfs3MSj4g0k2AnTbQ5OJKEnqY2ihV9WjwLeTnNUNbQXuA3YDc93YHLBrIgklSb2s73neHwGfSHIi8CDwbgYvBjcmuQL4FvCOyUSUJPXRq9Cr6m5gdolDW8cbR5I0Ku8UlaRGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhrR698UTfIQ8D3gaeBwVc0mOQW4AdgMPAT8blU9NpmYkqRhVjJDf2NVnVNVz/5j0TuAvVW1Bdjb7UuSpmQ1Sy6XAvPd43lg2+rjSJJG1bfQC/hckjuSbO/GNlTVIwDd9rRJBJQk9dNrDR24sKoeTnIasCfJ/X2foHsB2A5wxhlnjBBRktRHrxl6VT3cbQ8BNwMXAAeTbATotoeWuXZnVc1W1ezMzMx4UkuSfs7QQk/y4iQvefYx8JvAPcBuYK47bQ7YNamQkqTh+iy5bABuTvLs+Z+sqluS3A7cmOQK4FvAOyYXU5I0zNBCr6oHgbOXGP8usHUSoSRJK+edopLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJakTvQk+yLsldST7T7b8yyW1J9ie5IcmJk4spSRpmJTP0K4F9R+x/GLimqrYAjwFXjDOYJGllehV6kk3AJcBHu/0AbwJu6k6ZB7ZNIqAkqZ++M/S/B/4EeKbbfxnweFUd7vYPAKcvdWGS7UkWkiwsLi6uKqwkaXlDCz3JbwOHquqOI4eXOLWWur6qdlbVbFXNzszMjBhTkjTM+h7nXAi8LclvAS8AXspgxn5SkvXdLH0T8PDkYkqShhk6Q6+qP62qTVW1GbgM+HxVvRP4AvD27rQ5YNfEUkqShlrN+9A/ALw/yTcYrKlfO55IkqRR9Fly+amquhW4tXv8IHDB+CNJkkbhnaKS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWrE0EJP8oIk/53ky0nuTfKhbvyVSW5Lsj/JDUlOnHxcSdJy+szQfwS8qarOBs4BLk7yeuDDwDVVtQV4DLhicjElScMMLfQa+H63e0L3q4A3ATd14/PAtokklCT10msNPcm6JHcDh4A9wAPA41V1uDvlAHD6MtduT7KQZGFxcXEcmSVJS+hV6FX1dFWdA2wCLgBes9Rpy1y7s6pmq2p2ZmZm9KSSpOe0one5VNXjwK3A64GTkqzvDm0CHh5vNEnSSvR5l8tMkpO6xy8E3gzsA74AvL07bQ7YNamQkqTh1g8/hY3AfJJ1DF4AbqyqzyS5D7g+yV8BdwHXTjCnJGmIoYVeVV8Bzl1i/EEG6+mSpOOAd4pKUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRQws9ycuTfCHJviT3JrmyGz8lyZ4k+7vtyZOPK0laTp8Z+mHgj6vqNcDrgfckeS2wA9hbVVuAvd2+JGlKhhZ6VT1SVXd2j78H7ANOBy4F5rvT5oFtkwopSRpuRWvoSTYD5wK3ARuq6hEYlD5w2jLXbE+ykGRhcXFxdWklScvqXehJfgH4F+Cqqnqy73VVtbOqZqtqdmZmZpSMkqQeehV6khMYlPknqurT3fDBJBu74xuBQ5OJKEnqo8+7XAJcC+yrqr874tBuYK57PAfsGn88SVJf63uccyHwLuCrSe7uxj4IXA3cmOQK4FvAOyYTUZLUx9BCr6ovAVnm8NbxxpEkjco7RSWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGDC30JB9LcijJPUeMnZJkT5L93fbkycaUJA3TZ4b+T8DFR43tAPZW1RZgb7cvSZqioYVeVf8O/M9Rw5cC893jeWDbmHNJklZo1DX0DVX1CEC3PW18kSRJo5j4D0WTbE+ykGRhcXFx0k8nSf9vjVroB5NsBOi2h5Y7sap2VtVsVc3OzMyM+HSSpGFGLfTdwFz3eA7YNZ44kqRR9Xnb4nXAfwFnJTmQ5ArgauCiJPuBi7p9SdIUrR92QlVdvsyhrWPOIklaBe8UlaRGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSI1ZV6EkuTvK1JN9IsmNcoSRJKzdyoSdZB/wD8FbgtcDlSV47rmCSpJVZzQz9AuAbVfVgVf0YuB64dDyxJEkrlaoa7cLk7cDFVfUH3f67gNdV1XuPOm87sL3bPQv42uhxhzoV+M4Ev/6kreX8azk7mH/azP/cXlFVM8NOWr+KJ8gSYz/36lBVO4Gdq3ie3pIsVNXssXiuSVjL+ddydjD/tJl/PFaz5HIAePkR+5uAh1cXR5I0qtUU+u3AliSvTHIicBmwezyxJEkrNfKSS1UdTvJe4LPAOuBjVXXv2JKN5pgs7UzQWs6/lrOD+afN/GMw8g9FJUnHF+8UlaRGWOiS1AgLXZIasZr3oUs/leSfq+r3p52jryQXAFVVt3cfWXExcH9V/duUo+k4l+TVDO6KP53BvTcPA7urat9Ug7GGfyia5H3AzVX17WlnWakkrwP2VdWTSV4I7ADOA+4D/qaqnphqwCGSHP321ABvBD4PUFVvO+ahViDJXzD4DKL1wB7gdcCtwJuBz1bVX08vXT9dqZwO3FZV3z9i/OKqumV6ydqW5APA5Qw+6uRAN7yJwdu2r6+qq6eVDdZ2oT8B/AB4ALgO+FRVLU43VT9J7gXO7t76uRN4CrgJ2NqN/85UAw6R5E4GLz4fZTBDCYP/B5cBVNUXp5duuCRfBc4Bng88Cmw64sX1tqr61akGHKKbzLwH2Mfg93FlVe3qjt1ZVedNM99qJHl3VX182jmWk+TrwK9U1U+OGj8RuLeqtkwn2cBaXkN/kMEr418C5wP3JbklyVySl0w32lDPq6rD3ePZqrqqqr5UVR8CzpxmsJ5mgTuAPwOeqKpbgR9W1ReP9zLvHK6qp6vqKeCBqnoSoKp+CDwz3Wi9/CFwflVtA94A/HmSK7tjS30kx1ryoWkHGOIZ4JeWGN/IcfC9s5bX0KuqngE+B3wuyQkM/hp9OfC3wNAPspmie46YiXw5yWxVLSR5FfCTYRdPW/ff/Zokn+q2B1lb30s/TvKirtDPf3YwyS9yHPyh7GHds8ssVfVQkjcANyV5BWug0JN8ZblDwIZjmWUEVwF7k+wHnl3uPQP4ZeC9y151jKzlJZe7qurcZY69sJttHZe64vgI8OsMPqHtPAbfHN8G3ldVX55ivBVLcglwYVV9cNpZ+kjy/Kr60RLjpwIbq+qrU4jVW5LPA++vqruPGFsPfAx4Z1Wtm1q4HroJwFuAx44+BPxnVS01Az5uJHkeg48PP51B5gPA7VX19FSDsbYL/VVV9fVp51iNbmnoTAaz2wNVdXDKkbQGJNnEYNno0SWOXVhV/zGFWL0luRb4eFV9aYljn6yq35tCrCas2UKXJP2stfxDUUnSESx0SWqEhS5JjbDQJakR/wtUl+VzrhD34gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.relevancy.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the training corpus\n",
    "Make sure there are enough articles of each category represented in the training corpus. The training corpus will be used to mesure the score of the classfication results by using the manual assigned information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ddf05e15c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df.relevancy.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topic models using your training corpus <a class=\"anchor\" id=\"8-bullet\"></a>\n",
    "\n",
    "The function \"train_lda\" trains the lda model. You can change the parameters like number of topics or chunksize, but also the change of the alpha and eta parameters can change the results a lot. For the text classification, a high number of topics is best suited. Of course, this can change from research question to research question, and it makes sense to train your models with a changing number of topics to find out which amount works best for your collection.\n",
    "\n",
    "The program is doing also several passes of the data since this is a small dataset, so we want the distributions to stabilize. \n",
    "\n",
    "It is also important to note that changing the parameters may lead to better results for some categories but worse results for others. If an overall good result is important, the parameters should be adjusted accordingly. On the other hand, if a good result is important for certain categories, you can simply ignore the result of those you do not need. The score is calculated after the model has been trained and the collection classified. To find out, which parameters work the best for your corpus, you simply have to try out a view times and see what happens when you change the parameters. Every collection is different.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(train_df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a BOW for every document (Bag of words)\n",
    "def document_to_bow(df):\n",
    "    train_df['bow'] = list(map(lambda doc: dictionary.doc2bow(doc), train_df['tokenized']))\n",
    "    \n",
    "document_to_bow(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_df.bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(data):\n",
    "    num_topics = 500\n",
    "    t1 = time.time()\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                    minimum_probability=0.0, passes=50, iterations=300, per_word_topics=True)\n",
    "    return dictionary,corpus,lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training of the topic models takes a few minutes. But it is worth the waiting time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary,corpus,lda = train_lda(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at your topics <a class=\"anchor\" id=\"9-bullet\"></a>\n",
    "Inspect the outcome of your topics. You can see all your topics in changing the topicid to the number of topic you want to see. You can also adapt the number of tokens (topn) by changing the number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('einfind', 8.949347e-05),\n",
       " ('postenlos', 8.949347e-05),\n",
       " ('stadtbahnhaltestell', 8.949347e-05),\n",
       " ('vogelsang', 8.949347e-05),\n",
       " ('dreimonat', 8.949347e-05),\n",
       " ('einschreib', 8.949347e-05),\n",
       " ('gasb', 8.949347e-05),\n",
       " ('lchranfaht', 8.949347e-05),\n",
       " ('lefar', 8.949347e-05),\n",
       " ('padchr', 8.949347e-05),\n",
       " ('portngies', 8.949347e-05),\n",
       " ('schulkanzlei', 8.949347e-05),\n",
       " ('uep', 8.949347e-05),\n",
       " ('74134', 8.949347e-05),\n",
       " ('zimmerund', 8.949347e-05),\n",
       " ('formalitat', 8.949347e-05),\n",
       " ('gelandet', 8.949347e-05),\n",
       " ('aufruf', 8.949347e-05),\n",
       " ('intelligenzl', 8.949347e-05),\n",
       " ('intelligenzberuf', 8.949347e-05)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topic(topicid=0, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is time to create the feature vector <a class=\"anchor\" id=\"10-bullet\"></a>\n",
    "Freature vectore is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_lda_features(lda, document):\n",
    "    \"\"\" Transforms a bag of words document to features.\n",
    "    It returns the proportion of how much each topic was\n",
    "    present in the document.\n",
    "    \"\"\"\n",
    "    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n",
    "    topic_importances = np.array(topic_importances)\n",
    "    return topic_importances[:,1]\n",
    "\n",
    "train_df['lda_features'] = list(map(lambda doc:\n",
    "                                      document_to_lda_features(lda, doc),\n",
    "                                      train_df.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_topic_distribution = train_df.loc[train_df.relevancy == 0, 'lda_features'].mean()\n",
    "news_topic_distribution = train_df.loc[train_df.relevancy == 1, 'lda_features'].mean()\n",
    "culture_topic_distribution = train_df.loc[train_df.relevancy == 2, 'lda_features'].mean()\n",
    "appeals_topic_distribution = train_df.loc[train_df.relevancy == 3, 'lda_features'].mean()\n",
    "crime_topic_distribution = train_df.loc[train_df.relevancy == 4, 'lda_features'].mean()\n",
    "finances_topic_distribution = train_df.loc[train_df.relevancy == 6, 'lda_features'].mean()\n",
    "statistic_topic_distribution = train_df.loc[train_df.relevancy == 7, 'lda_features'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at the top words for each category <a class=\"anchor\" id=\"11-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_top_words(lda_model, topic_id, nr_top_words=5):\n",
    "    \"\"\" Returns the top words for topic_id from lda_model.\n",
    "    \"\"\"\n",
    "    id_tuples = lda_model.get_topic_terms(topic_id, topn=nr_top_words)\n",
    "    word_ids = np.array(id_tuples)[:,0]\n",
    "    words = map(lambda id_: lda_model.id2word[id_], word_ids)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up top words from top topics from ads.\n",
      "For topic 260, the top words are: franzos, englisch, deutsch, lern, palastina.\n",
      "For topic 304, the top words are: jahr, spanisch, englisch, verlust, italien.\n",
      "For topic 455, the top words are: wien, bezirk, hoh, markt, auskunft.\n",
      "For topic 463, the top words are: weg, verkauf, bez, billig, zimm.\n",
      "For topic 477, the top words are: sucht, deutsch, emigrant, abschliess, zeit.\n",
      "\n",
      "Looking up top words from top topics from news.\n",
      "For topic 24, the top words are: nationaldemokrati, warschau, april, russland, wurd.\n",
      "For topic 82, the top words are: land, tirol, fol, musst, kanzel.\n",
      "For topic 210, the top words are: fium, jahr, wurd, person, schiff.\n",
      "For topic 299, the top words are: irland, einwand, jahr, brasili, gorz.\n",
      "For topic 333, the top words are: osterreichischungar, dampf, unterseeboot, versenkt, italien.\n",
      "\n",
      "Looking up top words from top topics from culture.\n",
      "For topic 78, the top words are: zwei, ganz, jahr, damal, wurd.\n",
      "For topic 260, the top words are: franzos, englisch, deutsch, lern, palastina.\n",
      "For topic 334, the top words are: emigrant, berlin, bank, russischpol, theat.\n",
      "For topic 402, the top words are: deutsch, bohm, meer, abend, wurd.\n",
      "For topic 455, the top words are: wien, bezirk, hoh, markt, auskunft.\n",
      "\n",
      "Looking up top words from top topics from appeals.\n",
      "For topic 1, the top words are: auskunftskal, teil, les, zugeschickt, einsend.\n",
      "For topic 152, the top words are: amerika, amerikareis, 50, dollar, weit.\n",
      "For topic 187, the top words are: inn, auskunftsstell, anfrag, auskunft, wien.\n",
      "For topic 441, the top words are: press, freien, arm, neu, word.\n",
      "For topic 455, the top words are: wien, bezirk, hoh, markt, auskunft.\n",
      "\n",
      "Looking up top words from top topics from crime.\n",
      "For topic 243, the top words are: wurd, gross, erhielt, jung, liess.\n",
      "For topic 279, the top words are: gendarm, stellungspflicht, verhaft, wurd, zehn.\n",
      "For topic 362, the top words are: wurd, verhaftet, verhaft, militarpflicht, angehalt.\n",
      "For topic 489, the top words are: kron, zuschrift, jahr, hollandamerikalini, fall.\n",
      "For topic 498, the top words are: kopp, wurd, szek, word, wohnung.\n",
      "\n",
      "Looking up top words from top topics from finances.\n",
      "For topic 70, the top words are: deutsch, ausland, golddiskontbank, inland, bond.\n",
      "For topic 248, the top words are: amerikan, amerika, ausland, oesterreich, gebiet.\n",
      "For topic 381, the top words are: kron, million, jahr, geld, grundstuck.\n",
      "For topic 474, the top words are: wurd, decoration, oper, prinz, uhr.\n",
      "For topic 475, the top words are: berlin, ausland, erwerb, wertpapi, verausser.\n",
      "\n",
      "Looking up top words from top topics from statistic.\n",
      "For topic 42, the top words are: oesterreich, ruckgang, vorjahr, 1936, erst.\n",
      "For topic 236, the top words are: mehr, bevolker, gross, million, pol.\n",
      "For topic 365, the top words are: jahr, staat, vereinigt, uberseeisch, prozent.\n",
      "For topic 424, the top words are: deutschland, judisch, jud, jahr, rund.\n",
      "For topic 432, the top words are: 25, person, jahr, land, argentini.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for relevancy, distribution in zip(['ads', 'news', 'culture', 'appeals', 'crime', 'finances', 'statistic'], [ads_topic_distribution, news_topic_distribution, culture_topic_distribution, appeals_topic_distribution, crime_topic_distribution, finances_topic_distribution, statistic_topic_distribution]):\n",
    "    print(\"Looking up top words from top topics from {}.\".format(relevancy))\n",
    "    for x in sorted(np.argsort(distribution)[-5:]):\n",
    "        top_words = get_topic_top_words(lda, x)\n",
    "        print(\"For topic {}, the top words are: {}.\".format(x, \", \".join(top_words)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and hyperparameter tuning <a class=\"anchor\" id=\"12-bullet\"></a>\n",
    "After transforming the documents into features, it is important to apply a few supervised classifiers to be able to predict what text belongs to which category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_validated_model(model, param_grid, X, y, nr_folds=6):\n",
    "    \"\"\" Trains a model by doing a grid search combined with cross validation.\n",
    "    args:\n",
    "        model: your model\n",
    "        param_grid: dict of parameter values for the grid search\n",
    "    returns:\n",
    "        Model trained on entire dataset with hyperparameters chosen from best results in the grid search.\n",
    "    \"\"\"\n",
    "    # train the model (since the evaluation is based on the logloss, we'll use neg_log_loss here)\n",
    "    grid_cv = GridSearchCV(model, param_grid=param_grid, scoring='neg_log_loss', cv=nr_folds, n_jobs=-1, verbose=True)\n",
    "    best_model = grid_cv.fit(X, y)\n",
    "    # show top models with parameter values\n",
    "    result_df = pd.DataFrame(best_model.cv_results_)\n",
    "    show_columns = ['mean_test_score', 'rank_test_score']\n",
    "    for col in result_df.columns:\n",
    "        if col.startswith('param_'):\n",
    "            show_columns.append(col)\n",
    "    display(result_df[show_columns].sort_values(by='rank_test_score').head())\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first have to transform every entry\n",
    "X_train_lda = np.array(list(map(np.array, train_df.lda_features)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the test corpus  <a class=\"anchor\" id=\"13-bullet\"></a>\n",
    "First, have a look at your test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>iiif_url</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19110626_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1911-06-26T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>3</td>\n",
       "      <td>Blumenjunge, Meidling. Sie erhalten jetzt\\nim ...</td>\n",
       "      <td>[blumenj, meidling, erhalt, stellungspflicht, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19130305_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1913-03-05T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>3</td>\n",
       "      <td>J. F., Heumühlgasse.\\nPesen Sie die Artikel\\n...</td>\n",
       "      <td>[heumuhlgass, pes, artikel, amerikareis, ausku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19390921_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1939-09-21T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>4</td>\n",
       "      <td>Donnerstag, 21. September 1939\\n Das Ergebnis ...</td>\n",
       "      <td>[donnerstag, 21, septemb, 1939, ergebnis, vere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arbeiter_zeitung_aze19200205_article_81</td>\n",
       "      <td>de</td>\n",
       "      <td>1920-02-05T00:00:00Z</td>\n",
       "      <td>arbeiter_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/arbeiter_zeit...</td>\n",
       "      <td>4</td>\n",
       "      <td>* Das Opfer von Bauernfüngern. Der Arbeiter Ar...</td>\n",
       "      <td>[opf, bauernfung, arbeit, arthur, montag, wurd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>illustrierte_kronen_zeitung_krz19130630_articl...</td>\n",
       "      <td>de</td>\n",
       "      <td>1913-06-30T00:00:00Z</td>\n",
       "      <td>illustrierte_kronen_zeitung</td>\n",
       "      <td>https://platform.newseye.eu/iiif/illustrierte_...</td>\n",
       "      <td>4</td>\n",
       "      <td>(Der Raubmordversuch an einem Auswanderer.) Un...</td>\n",
       "      <td>[raubmordversuch, verdacht, raubmordversuch, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id language  \\\n",
       "0  illustrierte_kronen_zeitung_krz19110626_articl...       de   \n",
       "1  illustrierte_kronen_zeitung_krz19130305_articl...       de   \n",
       "2  illustrierte_kronen_zeitung_krz19390921_articl...       de   \n",
       "3            arbeiter_zeitung_aze19200205_article_81       de   \n",
       "4  illustrierte_kronen_zeitung_krz19130630_articl...       de   \n",
       "\n",
       "                   date                 newspaper_id  \\\n",
       "0  1911-06-26T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "1  1913-03-05T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "2  1939-09-21T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "3  1920-02-05T00:00:00Z             arbeiter_zeitung   \n",
       "4  1913-06-30T00:00:00Z  illustrierte_kronen_zeitung   \n",
       "\n",
       "                                            iiif_url  relevancy  \\\n",
       "0  https://platform.newseye.eu/iiif/illustrierte_...          3   \n",
       "1  https://platform.newseye.eu/iiif/illustrierte_...          3   \n",
       "2  https://platform.newseye.eu/iiif/illustrierte_...          4   \n",
       "3  https://platform.newseye.eu/iiif/arbeiter_zeit...          4   \n",
       "4  https://platform.newseye.eu/iiif/illustrierte_...          4   \n",
       "\n",
       "                                                text  \\\n",
       "0  Blumenjunge, Meidling. Sie erhalten jetzt\\nim ...   \n",
       "1   J. F., Heumühlgasse.\\nPesen Sie die Artikel\\n...   \n",
       "2  Donnerstag, 21. September 1939\\n Das Ergebnis ...   \n",
       "3  * Das Opfer von Bauernfüngern. Der Arbeiter Ar...   \n",
       "4  (Der Raubmordversuch an einem Auswanderer.) Un...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [blumenj, meidling, erhalt, stellungspflicht, ...  \n",
       "1  [heumuhlgass, pes, artikel, amerikareis, ausku...  \n",
       "2  [donnerstag, 21, septemb, 1939, ergebnis, vere...  \n",
       "3  [opf, bauernfung, arbeit, arthur, montag, wurd...  \n",
       "4  [raubmordversuch, verdacht, raubmordversuch, b...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process your test corpus using the same function than for the train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tokenized'] = test_df['text'].apply(apply_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a bag of words for every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_bow(df):\n",
    "    df['bow'] = list(map(lambda doc: dictionary.doc2bow(doc), test_df['tokenized']))\n",
    "    \n",
    "document_to_bow(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get feature vectores for your test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['lda_features'] = list(map(lambda doc:\n",
    "                                     document_to_lda_features(lda, doc),\n",
    "                                     test_df.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lda = np.array(list(map(np.array, test_df.lda_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all models in a dictionary\n",
    "models = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression <a class=\"anchor\" id=\"14-bullet\"></a>\n",
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.689019</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  rank_test_score param_penalty\n",
       "0        -1.689019                1            l2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "param_grid = {'penalty': ['l2']}\n",
    "\n",
    "best_lr_lda = get_cross_validated_model(lr, param_grid, X_train_lda, train_df.relevancy)\n",
    "\n",
    "models['best_lr_lda'] = best_lr_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is time to make the classifications  <a class=\"anchor\" id=\"15-bullet\"></a>\n",
    "First we get a data frame with the result for each category. The category with the highest number is the category to which the article is assigned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_predictions = best_lr_lda.predict_proba(X_test_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= np.append(test_df.relevancy.values.reshape(-1,1), submission_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(data=result, columns=['relevancy', 'ads', 'news', 'culture', 'appeals', 'crime', 'finance', 'statistic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look if everything worked correctly\n",
    "The first article contains the manual annotation (4.0), which means this article belongs to the category of crime. As you can see, the highest number for this row is in the column of crime. So this article has been classified correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevancy</th>\n",
       "      <th>ads</th>\n",
       "      <th>news</th>\n",
       "      <th>culture</th>\n",
       "      <th>appeals</th>\n",
       "      <th>crime</th>\n",
       "      <th>finance</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.094845</td>\n",
       "      <td>0.106635</td>\n",
       "      <td>0.084885</td>\n",
       "      <td>0.466985</td>\n",
       "      <td>0.092279</td>\n",
       "      <td>0.082374</td>\n",
       "      <td>0.071996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.109354</td>\n",
       "      <td>0.096412</td>\n",
       "      <td>0.110479</td>\n",
       "      <td>0.391502</td>\n",
       "      <td>0.101440</td>\n",
       "      <td>0.094285</td>\n",
       "      <td>0.096527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.119773</td>\n",
       "      <td>0.128907</td>\n",
       "      <td>0.128590</td>\n",
       "      <td>0.107295</td>\n",
       "      <td>0.218508</td>\n",
       "      <td>0.148261</td>\n",
       "      <td>0.148665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.126723</td>\n",
       "      <td>0.140767</td>\n",
       "      <td>0.166544</td>\n",
       "      <td>0.108862</td>\n",
       "      <td>0.174194</td>\n",
       "      <td>0.153799</td>\n",
       "      <td>0.129111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.116602</td>\n",
       "      <td>0.120878</td>\n",
       "      <td>0.106183</td>\n",
       "      <td>0.295672</td>\n",
       "      <td>0.109035</td>\n",
       "      <td>0.108530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevancy       ads      news   culture   appeals     crime   finance  \\\n",
       "0        3.0  0.094845  0.106635  0.084885  0.466985  0.092279  0.082374   \n",
       "1        3.0  0.109354  0.096412  0.110479  0.391502  0.101440  0.094285   \n",
       "2        4.0  0.119773  0.128907  0.128590  0.107295  0.218508  0.148261   \n",
       "3        4.0  0.126723  0.140767  0.166544  0.108862  0.174194  0.153799   \n",
       "4        4.0  0.143100  0.116602  0.120878  0.106183  0.295672  0.109035   \n",
       "\n",
       "   statistic  \n",
       "0   0.071996  \n",
       "1   0.096527  \n",
       "2   0.148665  \n",
       "3   0.129111  \n",
       "4   0.108530  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the score for each category as well as the overall score  <a class=\"anchor\" id=\"16-bullet\"></a>\n",
    "Repeat the calculation (topic models need sometimes several rounds) or adapt the code until you get a higher score than 80 percent. If you get a higher score than 80 percent, you can continue with your whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = submission_df.loc[:, submission_df.columns != 'relevancy'].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score for ads is: 0.8571428571428571\n",
      "Your score for news is: 0.7333333333333333\n",
      "Your score for culture is: 0.8333333333333334\n",
      "Your score for appeals is: 0.9444444444444444\n",
      "Your score for crime is: 1.0\n",
      "Your score for finances is: 0.6153846153846154\n",
      "Your score for statistic is: 0.7142857142857143\n",
      "Your overall score is 0.8139891854177569\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ads = submission_df[['ads', 'relevancy']][submission_df['ads'].isin(max_num)]\n",
    "\n",
    "ads_list = []\n",
    "for key, value in df_ads.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            ads_list.append(rel)\n",
    "\n",
    "ads_right = []\n",
    "ads_wrong =[]\n",
    "for num in ads_list: \n",
    "    if num == 0.0: \n",
    "        ads_right.append(num)\n",
    "    else: \n",
    "        ads_wrong.append(num)\n",
    "all_ = len(ads_right) + len(ads_wrong)\n",
    "ads_score = len(ads_right) / all_\n",
    "print(f\"Your score for ads is: {ads_score}\")\n",
    "\n",
    "df_news = submission_df[['news', 'relevancy']][submission_df['news'].isin(max_num)]\n",
    "\n",
    "\n",
    "news_list = []\n",
    "for key, value in df_news.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            news_list.append(rel)\n",
    "\n",
    "news_right = []\n",
    "news_wrong =[]\n",
    "for num in news_list: \n",
    "    if num == 1.0: \n",
    "        news_right.append(num)\n",
    "    else: \n",
    "        news_wrong.append(num)\n",
    "all_ = len(news_right) + len(news_wrong)\n",
    "news_score = len(news_right) / all_\n",
    "print(f\"Your score for news is: {news_score}\")\n",
    "\n",
    "    \n",
    "df_culture = submission_df[['culture', 'relevancy']][submission_df['culture'].isin(max_num)]\n",
    "\n",
    "culture_list = []\n",
    "for key, value in df_culture.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            culture_list.append(rel)\n",
    "\n",
    "culture_right = []\n",
    "culture_wrong =[]\n",
    "for num in culture_list: \n",
    "    if num == 2.0: \n",
    "        culture_right.append(num)\n",
    "    else: \n",
    "        culture_wrong.append(num)\n",
    "all_ = len(culture_right) + len(culture_wrong)\n",
    "culture_score = len(culture_right) / all_\n",
    "print(f\"Your score for culture is: {culture_score}\")\n",
    "\n",
    "df_appeals = submission_df[['appeals', 'relevancy']][submission_df['appeals'].isin(max_num)]\n",
    "\n",
    "appeals_list = []\n",
    "for key, value in df_appeals.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            appeals_list.append(rel)\n",
    "\n",
    "appeals_right = []\n",
    "appeals_wrong =[]\n",
    "for num in appeals_list: \n",
    "    if num == 3.0: \n",
    "        appeals_right.append(num)\n",
    "    else: \n",
    "        appeals_wrong.append(num)\n",
    "all_ = len(appeals_right) + len(appeals_wrong)\n",
    "appeals_score = len(appeals_right) / all_\n",
    "print(f\"Your score for appeals is: {appeals_score}\")\n",
    "\n",
    "df_crime = submission_df[['crime', 'relevancy']][submission_df['crime'].isin(max_num)]\n",
    "\n",
    "crime_list = []\n",
    "for key, value in df_crime.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            crime_list.append(rel)\n",
    "\n",
    "crime_right = []\n",
    "crime_wrong =[]\n",
    "for num in crime_list: \n",
    "    if num == 4.0: \n",
    "        crime_right.append(num)\n",
    "    else: \n",
    "        crime_wrong.append(num)\n",
    "all_ = len(crime_right) + len(crime_wrong)\n",
    "crime_score = len(crime_right) / all_\n",
    "print(f\"Your score for crime is: {crime_score}\")\n",
    "\n",
    "\n",
    "df_finances = submission_df[['finance', 'relevancy']][submission_df['finance'].isin(max_num)]\n",
    "\n",
    "finances_list = []\n",
    "for key, value in df_finances.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            finances_list.append(rel)\n",
    "\n",
    "finances_right = []\n",
    "finances_wrong =[]\n",
    "for num in finances_list: \n",
    "    if num == 6.0: \n",
    "        finances_right.append(num)\n",
    "    else: \n",
    "        finances_wrong.append(num)\n",
    "all_ = len(finances_right) + len(finances_wrong)\n",
    "finance_score = len(finances_right) / all_\n",
    "print(f\"Your score for finances is: {finance_score}\")\n",
    "   \n",
    "df_statistic = submission_df[['statistic', 'relevancy']][submission_df['statistic'].isin(max_num)]\n",
    "\n",
    "statistic_list = []\n",
    "for key, value in df_statistic.items(): \n",
    "    for rel in value: \n",
    "        if len(str(rel)) < 4:\n",
    "            statistic_list.append(rel)\n",
    "\n",
    "statistic_right = []\n",
    "statistic_wrong =[]\n",
    "for num in statistic_list: \n",
    "    if num == 7.0: \n",
    "        statistic_right.append(num)\n",
    "    else: \n",
    "        statistic_wrong.append(num)\n",
    "all_ = len(statistic_right) + len(statistic_wrong)\n",
    "statistic_score = len(statistic_right) / all_\n",
    "print(f\"Your score for statistic is: {statistic_score}\")\n",
    "overall_score = (ads_score + news_score + culture_score + appeals_score + crime_score + finance_score + statistic_score) / 7\n",
    "\n",
    "print(f\"Your overall score is {overall_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If your overall score is higher than 80 percent, you can start to use your whole collection  <a class=\"anchor\" id=\"17-bullet\"></a>\n",
    "\n",
    "Start with importing your whole collection. Import the same collection twice for the futher processing. \n",
    "\n",
    "Note: If you are mainly interested in one of the catecories, it makes sense to choose a model with a high score for that category. If you want a good overview of the distribution of the categories, a overall good score is more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'export_auswander_01_04_2021_23_08.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8c89e57d8fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'export_auswander_01_04_2021_23_08.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'newspaper_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'iiif_url'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_all_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'export_auswander_01_04_2021_23_08.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'newspaper_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'iiif_url'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'export_auswander_01_04_2021_23_08.csv'"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('export_auswander_01_04_2021_23_08.csv', usecols = ['id','language','date','newspaper_id','iiif_url','text'])\n",
    "df_all_2 = pd.read_csv('export_auswander_01_04_2021_23_08.csv', usecols = ['id','language','date','newspaper_id','iiif_url','text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean (pre-process) your whole collection <a class=\"anchor\" id=\"18-bullet\"></a>\n",
    "\n",
    "You repeat the same steps you did with your training and test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['tokenized'] = df_all['text'].apply(apply_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, have a look at your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get a list of all words\n",
    "all_words = [word for item in list(df_all['tokenized']) for word in item]\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(all_words)\n",
    "f\"The number of unique words is {len(fdist)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document length\n",
    "df_all['doc_len'] = df_all['tokenized'].apply(lambda x: len(x))\n",
    "doc_lengths = list(df_all['doc_len'])\n",
    "df_all.drop(labels='doc_len', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(f\"length of list: {len(doc_lengths)}\")\n",
    "print(f\"average document length: {np.average(doc_lengths)}\")\n",
    "print(f\"minimum document length: {min(doc_lengths)}\")\n",
    "print(f\"maximum document length: {max(doc_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove articles that are smaller than 5 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['tokenized'].map(len) >= 5]\n",
    "df_all = df_all[df_all['tokenized'].map(type) == list]\n",
    "df_all.reset_index(drop=True,inplace=True)\n",
    "print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df_all), \"articles\")\n",
    "\n",
    "df_all_2 = df_all[df_all['tokenized'].map(len) >= 5]\n",
    "df_all_2 = df_all[df_all['tokenized'].map(type) == list]\n",
    "df_all_2.reset_index(drop=True,inplace=True)\n",
    "print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df_all), \"articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a BOW (bag of words) for every document and get feature vectores for your whole collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_bow(df):\n",
    "    df['bow'] = list(map(lambda doc: dictionary.doc2bow(doc), df_all['tokenized']))\n",
    "    \n",
    "document_to_bow(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['lda_features'] = list(map(lambda doc:\n",
    "                                     document_to_lda_features(lda, doc),\n",
    "                                     df_all.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_lda = np.array(list(map(np.array, df_all.lda_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is time to make the classifications for the whole collection <a class=\"anchor\" id=\"19-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_predictions = best_lr_lda.predict_proba(X_all_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to create two different outputs, one with the results per category to check the result [57] and one with the results in the form of your original file [61], two different types of results are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= np.append(df_all.text.values.reshape(-1,1), submission_predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2= np.append(df_all_2, submission_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_all = pd.DataFrame(data=result, columns=['text', 'ads', 'news', 'culture', 'appeals', 'crime', 'finance', 'statistic'])\n",
    "submission_df_all_2 = pd.DataFrame(data=result_2, columns=['id','language','date','newspaper_id','iiif_url','text', 'token', 'ads', 'news', 'culture', 'appeals', 'crime', 'finance', 'statistic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe with the results <a class=\"anchor\" id=\"20-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = submission_df_all.loc[:, submission_df_all.columns != 'text'].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = submission_df_all[['text']][submission_df_all['ads'].isin(max_num)]\n",
    "news = submission_df_all[['text']][submission_df_all['news'].isin(max_num)]\n",
    "culture = submission_df_all[['text']][submission_df_all['culture'].isin(max_num)]\n",
    "appeals = submission_df_all[['text']][submission_df_all['appeals'].isin(max_num)]\n",
    "crime = submission_df_all[['text']][submission_df_all['crime'].isin(max_num)]\n",
    "finance = submission_df_all[['text']][submission_df_all['finance'].isin(max_num)]\n",
    "statistic = submission_df_all[['text']][submission_df_all['statistic'].isin(max_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#transform your lists into a dataframe\n",
    "\n",
    "df_ads = pd.DataFrame(np.column_stack([ads]), \n",
    "                               columns=['Ads'])\n",
    "\n",
    "\n",
    "df_news = pd.DataFrame(np.column_stack([news]), \n",
    "                               columns=['News'])\n",
    "\n",
    "df_culture = pd.DataFrame(np.column_stack([culture]), \n",
    "                               columns=['Culture_Literature'])\n",
    "\n",
    "df_appeals = pd.DataFrame(np.column_stack([appeals]), \n",
    "                               columns=['Appeals_Donations'])\n",
    "\n",
    "df_crime = pd.DataFrame(np.column_stack([crime]), \n",
    "                               columns=['Crime'])\n",
    "\n",
    "\n",
    "\n",
    "df_finance = pd.DataFrame(np.column_stack([finance]), \n",
    "                               columns=['Finance'])\n",
    "\n",
    "df_statistic = pd.DataFrame(np.column_stack([statistic]), \n",
    "                               columns=['Statistic'])\n",
    "\n",
    "df_results = pd.concat([df_ads, df_news, df_culture, df_appeals, df_crime, df_finance, df_statistic], ignore_index=True, axis=1)\n",
    "df_results.columns=['Ads','News', 'Culture_Literatur', 'Appeals_Donations', 'Crime', 'Finance', 'Statistic']\n",
    "df_results['Ads'][80:90] = df_results['Ads'][50:60].apply(lambda x: x[:200])\n",
    "df_results['News'][80:90] = df_results['News'][80:90].apply(lambda x: x[:200])\n",
    "df_results['Culture_Literatur'][80:90] = df_results['Culture_Literatur'][80:90].apply(lambda x: x[:200])\n",
    "df_results['Appeals_Donations'][80:90] = df_results['Appeals_Donations'][80:90].apply(lambda x: x[:200]) \n",
    "df_results['Crime'][80:90] = df_results['Crime'][80:90].apply(lambda x: x[:300])\n",
    "df_results['Finance'][80:90] = df_results['Finance'][80:90].apply(lambda x: x[:500])\n",
    "df_results['Statistic'][80:90] = df_results['Statistic'][80:90].apply(lambda x: x[:200])\n",
    "display(df_results[85:90].style.hide_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now export your dataframe in order to check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results.to_excel(\"results_emigration_8.xlsx\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dates for your classified articles\n",
    "You will need them later for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dates for the visualization for every category\n",
    "date_ads = []\n",
    "ads = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['ads'].isin(max_num)]\n",
    "date = submission_df_all_2['date'][submission_df_all['ads'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_ads.append(key[:4])\n",
    "# Transform the dates from strings to integers\n",
    "for i in range(0, len(date_ads)): \n",
    "    date_ads[i] = int(date_ads[i]) \n",
    "news = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['news'].isin(max_num)]\n",
    "date_news = []\n",
    "date = submission_df_all_2['date'][submission_df_all['news'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_news.append(key[:4])\n",
    "for i in range(0, len(date_news)): \n",
    "    date_news[i] = int(date_news[i])\n",
    "culture = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['culture'].isin(max_num)]\n",
    "date_culture = []\n",
    "date = submission_df_all_2['date'][submission_df_all['culture'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_culture.append(key[:4])\n",
    "for i in range(0, len(date_culture)): \n",
    "    date_culture[i] = int(date_culture[i])\n",
    "appeals = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['appeals'].isin(max_num)]\n",
    "date_appeals = []\n",
    "date = submission_df_all_2['date'][submission_df_all['appeals'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_appeals.append(key[:4])\n",
    "for i in range(0, len(date_appeals)): \n",
    "    date_appeals[i] = int(date_appeals[i])\n",
    "crime = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['crime'].isin(max_num)]\n",
    "date_crime = []\n",
    "date = submission_df_all_2['date'][submission_df_all['crime'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_crime.append(key[:4])\n",
    "for i in range(0, len(date_crime)): \n",
    "    date_crime[i] = int(date_crime[i])\n",
    "finance = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['finance'].isin(max_num)]\n",
    "date_finance = []\n",
    "date = submission_df_all_2['date'][submission_df_all['finance'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_finance.append(key[:4])\n",
    "for i in range(0, len(date_finance)): \n",
    "    date_finance[i] = int(date_finance[i])\n",
    "statistic = submission_df_all_2[['id','language','date','newspaper_id','iiif_url','text']][submission_df_all['statistic'].isin(max_num)]\n",
    "date_statistic = []\n",
    "date = submission_df_all_2['date'][submission_df_all['statistic'].isin(max_num)]\n",
    "for key in date: \n",
    "    date_statistic.append(key[:4])\n",
    "for i in range(0, len(date_statistic)): \n",
    "    date_statistic[i] = int(date_statistic[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are satisfied with the results, you can save them in the form of your original file <a class=\"anchor\" id=\"21-bullet\"></a>\n",
    "\n",
    "Otherwise, try to repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform your lists into a dataframe\n",
    "\n",
    "df_ads = pd.DataFrame(np.column_stack([ads]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n",
    "\n",
    "\n",
    "df_news = pd.DataFrame(np.column_stack([news]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n",
    "\n",
    "df_culture = pd.DataFrame(np.column_stack([culture]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n",
    "\n",
    "df_appeals = pd.DataFrame(np.column_stack([appeals]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n",
    "\n",
    "df_crime = pd.DataFrame(np.column_stack([crime]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n",
    "\n",
    "df_finance = pd.DataFrame(np.column_stack([finance]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n",
    "\n",
    "df_statistic = pd.DataFrame(np.column_stack([statistic]), \n",
    "                               columns=['id','language','date','newspaper_id','iiif_url','text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at your data\n",
    "you can create this for every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export your categorised collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_culture.to_csv('collection_culture.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your results <a class=\"anchor\" id=\"21-bullet\"></a>\n",
    "First, we are going to create a pie chart showing the distribution of every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(14, 8))\n",
    "# Data to plot\n",
    "labels = 'Advertisement', 'News', 'Culture_Literature_Stories_Letters', 'Appeals_Donations_Information', 'Crime', 'Finance', 'Statistic'\n",
    "sizes = [len(ads), len(news), len(culture), len(appeals), len(crime), len(finance), len(statistic)]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'blueviolet', 'darkturquoise', 'sandybrown', 'pink']\n",
    "explode = (0, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=False, startangle=14)\n",
    "plt.axis('equal', fontsize=100)\n",
    "plt.savefig('circle.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are creating histograms to find out which category was very popular at what time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.axes as ax\n",
    "plt.figure(figsize=(30, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(date_ads, color = 'gold', orientation=\"vertical\")\n",
    "plt.xlabel('Advertisements', size= 'x-large')\n",
    "plt.subplot(122)\n",
    "plt.hist(date_news, color = 'yellowgreen')\n",
    "plt.xlabel('News', size= 'x-large')\n",
    "plt.show()\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(date_culture, color = 'lightskyblue')\n",
    "plt.xlabel('Culture_Literature_Storie_Letters', size= 'x-large')\n",
    "plt.subplot(122)\n",
    "plt.hist(date_appeals, color = 'lightcoral')\n",
    "plt.xlabel('Appeals_Donations_Information', size= 'x-large')\n",
    "plt.show()\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(date_crime, color = 'sandybrown')\n",
    "plt.xlabel('Crime', size= 'x-large')\n",
    "plt.subplot(122)\n",
    "plt.hist(date_finance, color = 'pink')\n",
    "plt.xlabel('Finance', size= 'x-large')\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.hist(date_statistic, color = 'darkturquoise')\n",
    "plt.xlabel('Statistic', size= 'x-large')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
